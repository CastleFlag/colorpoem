{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730ecc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import GPUtil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a33903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9f8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef2bd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사람문장1</th>\n",
       "      <th>감정_대분류</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아내가 드디어 출산하게 되어서 정말 신이 나.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>빚을 드디어 다 갚게 되어서 이제야 안도감이 들어.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>이제 곧 은퇴할 시기가 되었어. 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>사십 대에 접어들면서 머리카락이 많이 빠져 고민이야.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>이제 돈이라면 지긋지긋해.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>친구들이 나를 괴롭혀. 부모님과 선생님께 얘기했는데도 믿어주지 않아.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>친구 때문에 눈물 나.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               사람문장1  감정_대분류\n",
       "0                          아내가 드디어 출산하게 되어서 정말 신이 나.       1\n",
       "1            당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.       3\n",
       "2            고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.       5\n",
       "3      재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.       1\n",
       "4                       빚을 드디어 다 갚게 되어서 이제야 안도감이 들어.       1\n",
       "5  이제 곧 은퇴할 시기가 되었어. 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를...       3\n",
       "6                      사십 대에 접어들면서 머리카락이 많이 빠져 고민이야.       2\n",
       "7                                     이제 돈이라면 지긋지긋해.       4\n",
       "8             친구들이 나를 괴롭혀. 부모님과 선생님께 얘기했는데도 믿어주지 않아.       4\n",
       "9                                       친구 때문에 눈물 나.       2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('../resource/emotion_classification.csv').dropna(axis=0)#.sample(frac=0.5)\n",
    "#중립0 행복1 슬픔2 공포3 분노4 놀람5 혐오6 상처7\n",
    "emodict = {'중립':0, '행복':1, '슬픔':2, '공포':3, '분노':4, '놀람':5, '혐오':6, '상처':7}\n",
    "for k,v in emodict.items():\n",
    "    dataframe = dataframe.replace(k, v)\n",
    "#상처 감정 삭제\n",
    "dataframe.drop(dataframe[dataframe['감정_대분류']==7].index, inplace=True)\n",
    "dataframe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1370658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    60577\n",
       "2    28430\n",
       "4    22794\n",
       "5    19875\n",
       "1    18148\n",
       "3    16912\n",
       "6    10026\n",
       "Name: 감정_대분류, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['감정_대분류'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd04680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe['사람문장1'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5317ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dataframe, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e69a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 64\n",
    "learning_rate =  5e-5\n",
    "epochs = 3\n",
    "seed_val = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea2506b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 0]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e138c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_df)\n",
    "test_dataset = MyDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a19c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac7b61ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7442f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "itr = 1\n",
    "p_itr = 500\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9426809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 47% | 15% |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2524fbc847344a7e890fcee3eb3ba7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\green\\anaconda3\\envs\\cp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\green\\AppData\\Local\\Temp\\ipykernel_9604\\4093315795.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(label)\n",
      "C:\\Users\\green\\AppData\\Local\\Temp\\ipykernel_9604\\4093315795.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3] Iteration 500 -> Train Loss: 1.6116, Accuracy: 0.405\n",
      "[Epoch 1/3] Iteration 1000 -> Train Loss: 1.2730, Accuracy: 0.545\n",
      "[Epoch 1/3] Iteration 1500 -> Train Loss: 1.1645, Accuracy: 0.586\n",
      "[Epoch 1/3] Iteration 2000 -> Train Loss: 1.1218, Accuracy: 0.599\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dd17d886de48149cdaada3ca28d641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\green\\AppData\\Local\\Temp\\ipykernel_9604\\4093315795.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(label)\n",
      "C:\\Users\\green\\AppData\\Local\\Temp\\ipykernel_9604\\4093315795.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6114049727038724\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 58% | 96% |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a28c72fd14845798ae0cbb347974efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/3] Iteration 2500 -> Train Loss: 1.0631, Accuracy: 0.629\n",
      "[Epoch 2/3] Iteration 3000 -> Train Loss: 1.0439, Accuracy: 0.628\n",
      "[Epoch 2/3] Iteration 3500 -> Train Loss: 1.0303, Accuracy: 0.627\n",
      "[Epoch 2/3] Iteration 4000 -> Train Loss: 1.0287, Accuracy: 0.632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5ec3f76ab940d09031d56d6222450c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6431703108647073\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 82% | 94% |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd661bd0ba5d457d995d375a298539b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/3] Iteration 4500 -> Train Loss: 1.0013, Accuracy: 0.666\n",
      "[Epoch 3/3] Iteration 5000 -> Train Loss: 0.9579, Accuracy: 0.657\n",
      "[Epoch 3/3] Iteration 5500 -> Train Loss: 0.9555, Accuracy: 0.658\n",
      "[Epoch 3/3] Iteration 6000 -> Train Loss: 0.9612, Accuracy: 0.654\n",
      "[Epoch 3/3] Iteration 6500 -> Train Loss: 0.9530, Accuracy: 0.658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9937c7a00c134e2a81fddb6159a3e272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6409074194552089\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    GPUtil.showUtilization()\n",
    "    model.train()\n",
    "    for batch_id, (text, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=max_len, pad_to_max_length=True) for t in text]\n",
    "        sample = torch.tensor(encoded_list)\n",
    "        labels = torch.tensor(label)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        labels = torch.tensor(label)\n",
    "        outputs = model(sample, labels=labels)\n",
    "        #loss, logits = outputs\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if itr % p_itr == 0:\n",
    "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "            total_loss = 0\n",
    "            total_len = 0\n",
    "            total_correct = 0\n",
    "    #torch.save(model.state_dict(), './model.pt')\n",
    "        itr+=1 \n",
    "     \n",
    "    model.eval()\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    for batch_id, (text, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=max_len, pad_to_max_length=True) for t in text]\n",
    "        sample = torch.tensor(encoded_list)\n",
    "        labels = torch.tensor(label)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        labels = torch.tensor(label)\n",
    "        with torch.no_grad():    \n",
    "            outputs = model(sample, labels=labels)\n",
    "        \n",
    "        logits = outputs[1]\n",
    "        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "    print(\"epoch {} test acc {}\".format(epoch+1, total_correct/total_len))\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6b941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cd40f3b",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
