{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730ecc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\green\\Desktop\\workspace\\colorpoem\\prototypes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import GPUtil\n",
    "import random\n",
    "%cd colorpoem/prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a33903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9f8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef2bd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사람문장1</th>\n",
       "      <th>감정_대분류</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아내가 드디어 출산하게 되어서 정말 신이 나.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>빚을 드디어 다 갚게 되어서 이제야 안도감이 들어.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>이제 곧 은퇴할 시기가 되었어. 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>사십 대에 접어들면서 머리카락이 많이 빠져 고민이야.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>이제 돈이라면 지긋지긋해.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>친구들이 나를 괴롭혀. 부모님과 선생님께 얘기했는데도 믿어주지 않아.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>친구 때문에 눈물 나.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               사람문장1  감정_대분류\n",
       "0                          아내가 드디어 출산하게 되어서 정말 신이 나.       1\n",
       "1            당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.       3\n",
       "2            고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.       5\n",
       "3      재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.       1\n",
       "4                       빚을 드디어 다 갚게 되어서 이제야 안도감이 들어.       1\n",
       "5  이제 곧 은퇴할 시기가 되었어. 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를...       3\n",
       "6                      사십 대에 접어들면서 머리카락이 많이 빠져 고민이야.       2\n",
       "7                                     이제 돈이라면 지긋지긋해.       4\n",
       "8             친구들이 나를 괴롭혀. 부모님과 선생님께 얘기했는데도 믿어주지 않아.       4\n",
       "9                                       친구 때문에 눈물 나.       2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('../resource/emotion_classification.csv').dropna(axis=0)#.sample(frac=0.2)\n",
    "#중립0 행복1 슬픔2 공포3 분노4 놀람5 혐오6 상처7\n",
    "emodict = {'중립':0, '행복':1, '슬픔':2, '공포':3, '분노':4, '놀람':5, '혐오':6, '상처':7}\n",
    "for k,v in emodict.items():\n",
    "    dataframe = dataframe.replace(k, v)\n",
    "dataframe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1370658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    60577\n",
       "2    28430\n",
       "4    22794\n",
       "5    19875\n",
       "1    18148\n",
       "3    16912\n",
       "6    10026\n",
       "7     7440\n",
       "Name: 감정_대분류, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['감정_대분류'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd04680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV5ElEQVR4nO3df4xd9Xnn8fezdiA0PzCEakRta+001lYO7g8yAlepsqN4Fwyp1qxEIrOouFkr1m6gTStXrWmkuk2CBLtL2CARKu/ajYmyMdRNhbVx1vECVxFSbX4EgjGUMAtObQtwgw10SBM67rN/3K+Tq8n92nN/eO6dyfslXc25z/mec77PnGE+3HPPHUdmIklSO/9i0BOQJA0vQ0KSVGVISJKqDAlJUpUhIUmqmj/oCfTbRRddlEuWLOl4uzfffJN3vOMd/Z/QANjLcLKX4TSXeoHu+3n88ce/n5k/P7U+50JiyZIlPPbYYx1v12g0GBsb6/+EBsBehpO9DKe51At0309EfK9d3ctNkqQqQ0KSVGVISJKqzhgSEbEtIo5FxNMttf8aEX8bEU9FxF9HxIKWdTdHxHhEPBcRV7bUV5faeERsaqkvjYj9pX5vRJxT6ueW5+Nl/ZJ+NS1Jmp7pvJL4ErB6Sm0vcElm/jLwXeBmgIhYDqwF3l+2+WJEzIuIecBdwFXAcuC6MhbgNuCOzHwfcAJYX+rrgROlfkcZJ0maQWcMicz8FnB8Su2bmTlZnu4DFpXlNcCOzPxRZr4IjAOXlcd4Zr6QmW8BO4A1ERHAh4GdZfvtwDUt+9pelncCq8p4SdIM6cctsP8RuLcsL6QZGqccKTWAw1PqlwPvAV5rCZzW8QtPbZOZkxHxehn//akTiIgNwAaAkZERGo1Gx01MTEx0td0wspfhZC/DaS71Av3vp6eQiIhPA5PAV/ozne5k5hZgC8Do6Gh2c4/wXLpX2l6Gk70Mp7nUC/S/n65DIiJ+G/hNYFX+5B+lOAosbhm2qNSo1F8FFkTE/PJqonX8qX0diYj5wPllvCRphnQVEhGxGvhD4F9n5g9aVu0C/ldEfB74BWAZ8AgQwLKIWErzl/9a4D9kZkbEQ8C1NN+nWAfc37KvdcDflPUP5ln+F5KWbPr62dz9aR269SMDO7Yk1ZwxJCLiq8AYcFFEHAE207yb6Vxgb3kveV9m/qfMPBgR9wHP0LwMdWNmniz7uQnYA8wDtmXmwXKIPwJ2RMTngCeAraW+FfhyRIzTfON8bR/6lSR14IwhkZnXtSlvbVM7Nf4W4JY29d3A7jb1F2je/TS1/kPgo2eanyTp7PET15KkKkNCklRlSEiSqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkKkNCklR1xpCIiG0RcSwinm6pXRgReyPi+fL1glKPiLgzIsYj4qmIuLRlm3Vl/PMRsa6l/oGIOFC2uTMi4nTHkCTNnOm8kvgSsHpKbRPwQGYuAx4ozwGuApaVxwbgbmj+wgc2A5cDlwGbW37p3w18omW71Wc4hiRphpwxJDLzW8DxKeU1wPayvB24pqV+TzbtAxZExMXAlcDezDyemSeAvcDqsu7dmbkvMxO4Z8q+2h1DkjRD5ne53UhmvlSWXwZGyvJC4HDLuCOldrr6kTb10x3jp0TEBpqvXBgZGaHRaHTYDkxMTLBxxcmOt+uXbuZcMzEx0df9DZK9DCd7GV797qfbkPixzMyIyH5MpttjZOYWYAvA6Ohojo2NdXyMRqPB7Q+/2fUce3Xo+rG+7avRaNDN92AY2ctwspfh1e9+ur276ZVyqYjy9VipHwUWt4xbVGqnqy9qUz/dMSRJM6TbkNgFnLpDaR1wf0v9hnKX00rg9XLJaA9wRURcUN6wvgLYU9a9EREry11NN0zZV7tjSJJmyBkvN0XEV4Ex4KKIOELzLqVbgfsiYj3wPeBjZfhu4GpgHPgB8HGAzDweEZ8FHi3jPpOZp94M/yTNO6jOA75RHpzmGJKkGXLGkMjM6yqrVrUZm8CNlf1sA7a1qT8GXNKm/mq7Y0iSZo6fuJYkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkKkNCklRlSEiSqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqSqnkIiIn4/Ig5GxNMR8dWIeHtELI2I/RExHhH3RsQ5Zey55fl4Wb+kZT83l/pzEXFlS311qY1HxKZe5ipJ6lzXIRERC4HfBUYz8xJgHrAWuA24IzPfB5wA1pdN1gMnSv2OMo6IWF62ez+wGvhiRMyLiHnAXcBVwHLgujJWkjRDer3cNB84LyLmAz8HvAR8GNhZ1m8HrinLa8pzyvpVERGlviMzf5SZLwLjwGXlMZ6ZL2TmW8COMlaSNEPmd7thZh6NiP8G/B3wj8A3gceB1zJzsgw7AiwsywuBw2XbyYh4HXhPqe9r2XXrNoen1C9vN5eI2ABsABgZGaHRaHTcz8TEBBtXnOx4u37pZs41ExMTfd3fINnLcLKX4dXvfroOiYi4gOb/2S8FXgP+kublohmXmVuALQCjo6M5NjbW8T4ajQa3P/xmn2c2fYeuH+vbvhqNBt18D4aRvQwnexle/e6nl8tN/wZ4MTP/PjP/Cfga8EFgQbn8BLAIOFqWjwKLAcr684FXW+tTtqnVJUkzpJeQ+DtgZUT8XHlvYRXwDPAQcG0Zsw64vyzvKs8p6x/MzCz1teXup6XAMuAR4FFgWblb6hyab27v6mG+kqQO9fKexP6I2Al8G5gEnqB5yefrwI6I+FypbS2bbAW+HBHjwHGav/TJzIMRcR/NgJkEbszMkwARcROwh+adU9sy82C385Ukda7rkADIzM3A5inlF2jemTR17A+Bj1b2cwtwS5v6bmB3L3OUJHXPT1xLkqoMCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkKkNCklRlSEiSqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSVU8hERELImJnRPxtRDwbEb8eERdGxN6IeL58vaCMjYi4MyLGI+KpiLi0ZT/ryvjnI2JdS/0DEXGgbHNnREQv85UkdabXVxJfAP5PZv4S8CvAs8Am4IHMXAY8UJ4DXAUsK48NwN0AEXEhsBm4HLgM2HwqWMqYT7Rst7rH+UqSOtB1SETE+cCHgK0AmflWZr4GrAG2l2HbgWvK8hrgnmzaByyIiIuBK4G9mXk8M08Ae4HVZd27M3NfZiZwT8u+JEkzYH4P2y4F/h74i4j4FeBx4FPASGa+VMa8DIyU5YXA4Zbtj5Ta6epH2tR/SkRsoPnqhJGRERqNRsfNTExMsHHFyY6365du5lwzMTHR1/0Nkr0MJ3sZXv3up5eQmA9cCvxOZu6PiC/wk0tLAGRmRkT2MsHpyMwtwBaA0dHRHBsb63gfjUaD2x9+s88z68CB/h1744qT0+7l0K0f6dtxz4ZGo0E353MY2ctwmku9QP/76eU9iSPAkczcX57vpBkar5RLRZSvx8r6o8Dilu0Xldrp6ova1CVJM6TrkMjMl4HDEfGvSmkV8AywCzh1h9I64P6yvAu4odzltBJ4vVyW2gNcEREXlDesrwD2lHVvRMTKclfTDS37kiTNgF4uNwH8DvCViDgHeAH4OM3guS8i1gPfAz5Wxu4GrgbGgR+UsWTm8Yj4LPBoGfeZzDxelj8JfAk4D/hGeUiSZkhPIZGZTwKjbVatajM2gRsr+9kGbGtTfwy4pJc5SpK65yeuJUlVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkKkNCklRlSEiSqgwJSVKVISFJqjIkJElVhoQkqarnkIiIeRHxRET87/J8aUTsj4jxiLg3Is4p9XPL8/GyfknLPm4u9eci4sqW+upSG4+ITb3OVZLUmX68kvgU8GzL89uAOzLzfcAJYH2prwdOlPodZRwRsRxYC7wfWA18sQTPPOAu4CpgOXBdGStJmiE9hURELAI+AvzP8jyADwM7y5DtwDVleU15Tlm/qoxfA+zIzB9l5ovAOHBZeYxn5guZ+Rawo4yVJM2Q+T1u/9+BPwTeVZ6/B3gtMyfL8yPAwrK8EDgMkJmTEfF6Gb8Q2Neyz9ZtDk+pX95uEhGxAdgAMDIyQqPR6LiRiYkJNq442fF2w2jkPNi4YvLMA6Gr79VMmpiYGPo5Tpe9DKe51Av0v5+uQyIifhM4lpmPR8RY32bUhczcAmwBGB0dzbGxzqfTaDS4/eE3+zyzwdi4YpLbD0zv1B66fuzsTqZHjUaDbs7nMLKX4TSXeoH+99PLK4kPAv8uIq4G3g68G/gCsCAi5pdXE4uAo2X8UWAxcCQi5gPnA6+21E9p3aZWlyTNgK7fk8jMmzNzUWYuofnG84OZeT3wEHBtGbYOuL8s7yrPKesfzMws9bXl7qelwDLgEeBRYFm5W+qccoxd3c5XktS5Xt+TaOePgB0R8TngCWBrqW8FvhwR48Bxmr/0ycyDEXEf8AwwCdyYmScBIuImYA8wD9iWmQfPwnwlSRV9CYnMbACNsvwCzTuTpo75IfDRyva3ALe0qe8GdvdjjpKkzvmJa0lSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkKkNCklRlSEiSqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSarqOiQiYnFEPBQRz0TEwYj4VKlfGBF7I+L58vWCUo+IuDMixiPiqYi4tGVf68r45yNiXUv9AxFxoGxzZ0REL81KkjrTyyuJSWBjZi4HVgI3RsRyYBPwQGYuAx4ozwGuApaVxwbgbmiGCrAZuBy4DNh8KljKmE+0bLe6h/lKkjrUdUhk5kuZ+e2y/A/As8BCYA2wvQzbDlxTltcA92TTPmBBRFwMXAnszczjmXkC2AusLuvenZn7MjOBe1r2JUmaAfP7sZOIWAL8GrAfGMnMl8qql4GRsrwQONyy2ZFSO139SJt6u+NvoPnqhJGRERqNRsc9TExMsHHFyY63G0Yj58HGFZPTGtvN92omTUxMDP0cp8tehtNc6gX630/PIRER7wT+Cvi9zHyj9W2DzMyIyF6PcSaZuQXYAjA6OppjY2Md76PRaHD7w2/2eWaDsXHFJLcfmN6pPXT92NmdTI8ajQbdnM9hZC/DaS71Av3vp6e7myLibTQD4iuZ+bVSfqVcKqJ8PVbqR4HFLZsvKrXT1Re1qUuSZkgvdzcFsBV4NjM/37JqF3DqDqV1wP0t9RvKXU4rgdfLZak9wBURcUF5w/oKYE9Z90ZErCzHuqFlX5KkGdDL5aYPAr8FHIiIJ0vtj4FbgfsiYj3wPeBjZd1u4GpgHPgB8HGAzDweEZ8FHi3jPpOZx8vyJ4EvAecB3ygPSdIM6TokMvNhoPa5hVVtxidwY2Vf24BtbeqPAZd0O0dJUm/8xLUkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkqq7/jWvNDUs2fX0gxz1060cGclxJnfGVhCSpypCQJFUZEpKkKkNCklRlSEiSqgwJSVLV0IdERKyOiOciYjwiNg16PpL0s2SoPycREfOAu4B/CxwBHo2IXZn5zGBnpl5N9/MZG1dM8tt9/iyHn9GQpm/YX0lcBoxn5guZ+RawA1gz4DlJ0s+MoX4lASwEDrc8PwJcPnVQRGwANpSnExHxXBfHugj4fhfbDZ3ftZfTitv6ubeOzJnzgr0Ms277+ZftisMeEtOSmVuALb3sIyIey8zRPk1poOxlONnLcJpLvUD/+xn2y01HgcUtzxeVmiRpBgx7SDwKLIuIpRFxDrAW2DXgOUnSz4yhvtyUmZMRcROwB5gHbMvMg2fpcD1drhoy9jKc7GU4zaVeoM/9RGb2c3+SpDlk2C83SZIGyJCQJFUZEsz+P/0REYci4kBEPBkRj5XahRGxNyKeL18vGPQ824mIbRFxLCKebqm1nXs03VnO01MRcengZv7TKr38aUQcLefmyYi4umXdzaWX5yLiysHMur2IWBwRD0XEMxFxMCI+Veqz7tycppdZd24i4u0R8UhEfKf08melvjQi9pc531tu9CEizi3Px8v6JR0fNDN/ph803xD/f8B7gXOA7wDLBz2vDns4BFw0pfZfgE1leRNw26DnWZn7h4BLgafPNHfgauAbQAArgf2Dnv80evlT4A/ajF1eftbOBZaWn8F5g+6hZX4XA5eW5XcB3y1znnXn5jS9zLpzU76/7yzLbwP2l+/3fcDaUv9z4D+X5U8Cf16W1wL3dnpMX0nM3T/9sQbYXpa3A9cMbip1mfkt4PiUcm3ua4B7smkfsCAiLp6RiU5DpZeaNcCOzPxRZr4IjNP8WRwKmflSZn67LP8D8CzNv4Aw687NaXqpGdpzU76/E+Xp28ojgQ8DO0t96nk5db52AqsiIjo5piHR/k9/nO4HaBgl8M2IeLz8iRKAkcx8qSy/DIwMZmpdqc19tp6rm8olmG0tl/1mTS/lEsWv0fy/1ll9bqb0ArPw3ETEvIh4EjgG7KX5Sue1zJwsQ1rn++NeyvrXgfd0cjxDYm74jcy8FLgKuDEiPtS6MpuvNWflvc6zee7F3cAvAr8KvATcPtDZdCgi3gn8FfB7mflG67rZdm7a9DIrz01mnszMX6X5FyguA37pbB7PkJgDf/ojM4+Wr8eAv6b5g/PKqZf75euxwc2wY7W5z7pzlZmvlP+o/xn4H/zkssXQ9xIRb6P5S/Urmfm1Up6V56ZdL7P53ABk5mvAQ8Cv07y8d+rD0a3z/XEvZf35wKudHMeQmOV/+iMi3hER7zq1DFwBPE2zh3Vl2Drg/sHMsCu1ue8Cbih30qwEXm+59DGUplyX//c0zw00e1lb7j5ZCiwDHpnp+dWU69ZbgWcz8/Mtq2bduan1MhvPTUT8fEQsKMvn0fy3dp6lGRbXlmFTz8up83Ut8GB5BTh9g363fhgeNO/M+C7Na3ufHvR8Opz7e2neifEd4OCp+dO87vgA8Dzwf4ELBz3Xyvy/SvOl/j/RvJa6vjZ3mnd23FXO0wFgdNDzn0YvXy5zfar8B3txy/hPl16eA64a9Pyn9PIbNC8lPQU8WR5Xz8Zzc5peZt25AX4ZeKLM+WngT0r9vTSDbBz4S+DcUn97eT5e1r+302P6ZzkkSVVebpIkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVX/H0aeJY74aS/3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe['사람문장1'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5317ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dataframe, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e69a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 64\n",
    "learning_rate =  5e-5\n",
    "epochs = 5\n",
    "seed_val = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca4b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = []\n",
    "# for index, row in tqdm(dataframe[:20].iterrows(), total=dataframe.shape[0]):\n",
    "#     text = row[0]\n",
    "#     encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True) for t in text]\n",
    "#     test.append(encoded_list)\n",
    "# newdf = pd.DataFrame({'사람문장' : test, '감정_대분류' :dataframe['감정_대분류'][:20]}).reset_index(drop=True)\n",
    "# print(newdf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2506b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        total_list = []\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            text = row[0]\n",
    "            encoded_list = tokenizer.encode(text, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True)\n",
    "            total_list.append(encoded_list)\n",
    "        self.df = pd.DataFrame({'사람문장' : total_list, '감정_대분류' :df['감정_대분류']}).reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = np.array(self.df.iloc[idx, 0])\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e138c93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b479b13282584b74b6002f2fcb12b14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84e8305cce143da9592b62f580eaeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_df)\n",
    "test_dataset = MyDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a19c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    text, label = data[0], data[1] \n",
    "    return torch.tensor(text), torch.tensor(label)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f084e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   101,   9708, 119235,  ...,      0,      0,      0],\n",
       "         [   101,   9303,  11287,  ...,      0,      0,      0],\n",
       "         [   101,   9011,    117,  ...,      0,      0,      0],\n",
       "         ...,\n",
       "         [   101,  58466,   9663,  ...,      0,      0,      0],\n",
       "         [   101,   9638,  17730,  ...,      0,      0,      0],\n",
       "         [   101,   9524, 119118,  ...,      0,      0,      0]],\n",
       "        dtype=torch.int32),\n",
       " tensor([0, 5, 0, 4, 2, 5, 4, 2, 1, 0, 3, 0, 0, 2, 0, 5, 0, 6, 5, 4, 0, 2, 0, 5,\n",
       "         2, 4, 5, 1, 5, 4, 0, 0, 5, 1, 4, 0, 0, 4, 7, 3, 0, 5, 5, 2, 4, 0, 4, 5,\n",
       "         0, 0, 4, 2, 0, 4, 1, 5, 5, 6, 4, 0, 3, 6, 1, 6])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac7b61ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7442f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "p_itr = 500\n",
    "itr=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fb14f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(logits, label):\n",
    "    top_logits = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "    correct = top_logits.eq(label).sum()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77c4039e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   101,   9665,  17655,   9638,  11287,   9519,  46150,  12424,\n",
      "         9539,  10622,   9266,  11664,  60030,   8989,  36908,   9523,\n",
      "        16985,    119,  77039,   9519,  28396,  25503,   9637,  49636,\n",
      "         9312,  32158,  33077,  12605, 118867,  12508,  12092,  45593,\n",
      "         9715, 119230,  10739,   8982,  77884,    119,    102,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0]), 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9426809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e28e96c0654fc3a6861694ca1c25ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] itr 500 -> Train Loss: 1.5740, Accuracy: 0.438\n",
      "[Epoch 1/5] itr 1000 -> Train Loss: 1.4159, Accuracy: 0.499\n",
      "[Epoch 1/5] itr 1500 -> Train Loss: 1.3421, Accuracy: 0.525\n",
      "[Epoch 1/5] itr 2000 -> Train Loss: 1.2980, Accuracy: 0.541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac382ca5279a4827adf4319111d17977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.5973578559027778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45493d7ec8ea41cfbf2e248bee3e9a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/5] itr 2500 -> Train Loss: 1.0713, Accuracy: 0.626\n",
      "[Epoch 2/5] itr 3000 -> Train Loss: 1.0662, Accuracy: 0.624\n",
      "[Epoch 2/5] itr 3500 -> Train Loss: 1.0637, Accuracy: 0.623\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gijeong/workspace/colorpoem/prototypes/multi_detete_injurylabel copy.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/gijeong/workspace/colorpoem/prototypes/multi_detete_injurylabel%20copy.ipynb#ch0000013?line=20'>21</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/gijeong/workspace/colorpoem/prototypes/multi_detete_injurylabel%20copy.ipynb#ch0000013?line=21'>22</a>\u001b[0m \u001b[39m# print(logits)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/Users/gijeong/workspace/colorpoem/prototypes/multi_detete_injurylabel%20copy.ipynb#ch0000013?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/gijeong/workspace/colorpoem/prototypes/multi_detete_injurylabel%20copy.ipynb#ch0000013?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/gijeong/workspace/colorpoem/prototypes/multi_detete_injurylabel%20copy.ipynb#ch0000013?line=25'>26</a>\u001b[0m acc \u001b[39m=\u001b[39m categorical_accuracy(logits, label) \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # GPUtil.showUtilization()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch_id, (text, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        #encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True) for t in text]\n",
    "        # print(text)\n",
    "        # print('-------------')\n",
    "        sample = text.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs = model(sample, labels=label)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        # print(logits)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = categorical_accuracy(logits, label) \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        if itr % p_itr == 0:\n",
    "            print('[Epoch {}/{}] itr {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, epoch_loss/(batch_id+1), epoch_acc/((1+batch_id)*batch_size)))\n",
    "        #torch.save(model.state_dict(), './model.pt')\n",
    "        itr+=1 \n",
    "     \n",
    "    model.eval()\n",
    "    test_acc = 0\n",
    "    for batch_id, (text, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        sample = text.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():    \n",
    "            outputs = model(sample, labels=label)\n",
    "        logits = outputs[1]\n",
    "        acc = categorical_accuracy(logits, label)\n",
    "        test_acc += acc.item()\n",
    "    print(\"epoch {} test acc {}\".format(epoch+1, test_acc/(batch_size*(batch_id+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    df = pd.DataFrame({'사람문장':[sentence], '감정_대분류':[0]})\n",
    "    #print(df)\n",
    "    dataset = MyDataset(df)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (text, label) in tqdm(enumerate(dataloader), total=len(dataloader)): \n",
    "        sample = text.to(device)\n",
    "        with torch.no_grad():    \n",
    "            outputs = model(sample)\n",
    "        logits = outputs[0]\n",
    "        return [k for k, v in emodict.items() if v == torch.argmax(logits)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa82606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec4dd6885f74cd4b80b7bb445fe9469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5255d9dde44913a858dda840be7661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'중립'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"너는 누구에게 한번이라도 뜨거운 사람이었느냐\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd40f3b",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfa0cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
